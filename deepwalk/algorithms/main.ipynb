{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "This is DeepWalk implementation by Danbi with Karate dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required Libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import graph\n",
    "from language_model import Skipgram\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab\n",
    "from multiprocessing import cpu_count\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepWalk process\n",
    "def deepwalk_process(args):\n",
    "\n",
    "  start_time = time.time()#processing time measurement\n",
    "\n",
    "  if args.format == \"adjacency\":\n",
    "    graph_adjacency = text_to_adjacency(args.input)\n",
    "    G = graph.Graph(graph_adjacency)#graph object\n",
    "    \n",
    "  print(\"\\nNumber of nodes: {}\".format(G.num_nodes))\n",
    "\n",
    "  num_walks = G.num_of_nodes * args.number_walks\n",
    "    \n",
    "  print(\"\\nNumber of walks: {}\".format(num_walks))\n",
    "    \n",
    "  data_size = num_walks * args.walks_length\n",
    "\n",
    "  print(\"\\nData size (walks*length): {}\".format(data_size))\n",
    "    \n",
    "  print(\"\\nWalking...\")\n",
    "  walks = G.build_deep_walk(G, num_paths=args.number_walks, path_length=args.walks_length, \n",
    "                            alpha=0, rand=random.Random(args.seed))\n",
    "  \n",
    "  print(\"\\nCounting vertex frequency...\")\n",
    "  vertex_counts = count_words(walks)# dictionary\n",
    "\n",
    "  print(\"\\nTraining...\")\n",
    "  if args.model == 'skipgram':\n",
    "    language_model = Skipgram(sentences=walks, vocabulary_counts=vertex_counts,size=args.dimension,\n",
    "                     window=args.window_size, min_count=0, trim_rule=None, workers=cpu_count(), iteration=args.iter)\n",
    "  else\n",
    "    raise Exception('language model is not Skipgram')\n",
    "    \n",
    "  total_time = time.time() - start_time()\n",
    "\n",
    "  print(\"\\nTraining completed\")\n",
    "  print(\"\\nembeddings has been generated\")\n",
    "  language_model.wv.save_word2vec_format(args.output)\n",
    "  print(\"\\nProcessing time: {}\".format(total_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(walks):# to count how many time the words appear in walks\n",
    "  c = Counter()\n",
    "\n",
    "  for words in walks:\n",
    "    c.update(words)\n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<34x34 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 190 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_adjacency(input_graph_file):#change the arg at the end \n",
    "    with open(input_graph_file, 'r') as f: \n",
    "\n",
    "#         lines = f.readlines()\n",
    "#         print(lines)\n",
    "        \n",
    "        num_lines= sum(1 for line in f)#number of nodes\n",
    "        print(num_lines)\n",
    "        graph = []\n",
    "            \n",
    "        for i in range(num_lines):#create 34 * 34 0 entry list\n",
    "            graph.append([0]*num_lines)\n",
    "            \n",
    "    with open(\"karate.adjacency\", 'r') as f:      \n",
    "        line_num = 0\n",
    "        for line in f.readlines():\n",
    "            nodes = line.split(\" \")\n",
    "            #print(nodes)\n",
    "            for j in range(len(nodes)):\n",
    "                n = int(nodes[j]) - 1\n",
    "                graph[line_num][n] = 1\n",
    "                #print(line_num, n)\n",
    "            line_num += 1\n",
    "    \n",
    "    sparse_matrix = sp.csr_matrix(graph)#sparse_matrix\n",
    "    \n",
    "    #G = nx.from_scipy_sparse_matrix(sparse_matrix)\n",
    "    #print(\"{}\".format(sparse_matrix))\n",
    "    return sparse_matrix\n",
    "\n",
    "text_to_adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--format FORMAT] --input [INPUT] --output\n",
      "                             OUTPUT [--num-walks NUM_WALKS]\n",
      "                             [--walk-length WALK_LENGTH]\n",
      "                             [--dimension DIMENSION] [--iter ITER]\n",
      "                             [--model MODEL]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --input, --output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soletop/Desktop/danbi_study/MY_Deepwalk/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "#argument parser\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--format', default='adjacency')#format of input graph file\n",
    "  parser.add_argument('--input', nargs='?', required=True, help=\"input graph file\")#input graph file\n",
    "  parser.add_argument('--number-walks', default=10, type=int)#walk length\n",
    "  parser.add_argument('--walks-length', default=40, type=int)#window size\n",
    "  parser.add_argument('--window-size', default=5, type=int, help='Window size')\n",
    "  parser.add_argument('--dimension', type=int, default=64, help='Embeddings dimension(size)')\n",
    "  parser.add_argument('--iter', default=1, type=int, help='Number of epochs in SGD')\n",
    "  parser.add_argument('--model', default='word2vec', help='language modeling(skipgram)')\n",
    "  parser.add_argument('--seed', default=0, type=int, help='Random seed for random walk')\n",
    "  parser.add_argument('--output', required=True, help=\"output embeddings file\")\n",
    "  #add argument for \"window_size=5\"\n",
    "    \n",
    "  args = parser.parse_args()\n",
    "\n",
    "  deepwalk_process(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
